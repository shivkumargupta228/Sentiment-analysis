# Import libraries
import bz2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from google.colab import files
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
import zipfile
import wget
import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
nltk.download('stopwords')

# Initialize the PorterStemmer and stopwords
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

# Preprocess text function (all in one)
def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\d+', '', text)
    text = ' '.join(text.split()).lower()
    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])
    return text

# Plotting functions
def plot_label_distribution(labels, title):
    unique, counts = np.unique(labels, return_counts=True)
    plt.bar(unique, counts, tick_label=['Negative', 'Positive'])
    plt.title(title)
    plt.xlabel('Label')
    plt.ylabel('Count')
    plt.show()

def plot_history(history):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Upload kaggle.json
print("Upload your kaggle.json file")
uploaded = files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# Kaggle API setup
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()
print("Kaggle API authentication successful.")

# Download dataset
!kaggle datasets download -d bittlingmayer/amazonreviews
!unzip -o amazonreviews.zip

# Download FastText embeddings
fasttext_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz'
wget.download(fasttext_url, 'cc.en.300.vec.gz')
!gunzip cc.en.300.vec.gz

# Label processing
def process_label(line):
    if line.startswith("_label_2 "):
        return 1, line[len("_label_2 "):]
    elif line.startswith("_label_1 "):
        return 0, line[len("_label_1 "):]
    return None, line

# Load data
def load_data(filename, max_samples=None):
    texts, labels = [], []
    with bz2.open(filename, 'rt', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if max_samples and i >= max_samples:
                break
            label, text = process_label(line.strip())
            if label is not None:
                texts.append(preprocess_text(text))
                labels.append(label)
    return texts, labels

# Load and preprocess data
train_texts, train_labels = load_data('train.ft.txt.bz2', max_samples=2000000)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.2, random_state=42)

# Plot label distributions
plot_label_distribution(train_labels, 'Training Set Label Distribution')
plot_label_distribution(val_labels, 'Validation Set Label Distribution')

# Tokenize and pad sequences
MAX_LEN = 100
tokenizer = Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(train_texts)
vocab_size = len(tokenizer.word_index) + 1
train_data = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=MAX_LEN)
val_data = pad_sequences(tokenizer.texts_to_sequences(val_texts), maxlen=MAX_LEN)
train_labels = np.array(train_labels)
val_labels = np.array(val_labels)

# Load FastText embeddings
embeddings_index = {}
with open('cc.en.300.vec', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        if len(values) < 2:
            continue
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
print(f'Found {len(embeddings_index)} word vectors.')

# Create embedding matrix
embedding_matrix = np.zeros((vocab_size, 300))
for word, i in tokenizer.word_index.items():
    vector = embeddings_index.get(word)
    if vector is not None:
        embedding_matrix[i] = vector

# Define model
def create_model():
    input_layer = tf.keras.layers.Input(shape=(MAX_LEN,))
    embedding = tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False)(input_layer)
    conv1 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(embedding)
    conv2 = tf.keras.layers.Conv1D(128, 6, activation='relu', padding='same')(embedding)
    conv3 = tf.keras.layers.Conv1D(128, 9, activation='relu', padding='same')(embedding)
    pool1 = tf.keras.layers.GlobalMaxPooling1D()(conv1)
    pool2 = tf.keras.layers.GlobalMaxPooling1D()(conv2)
    pool3 = tf.keras.layers.GlobalMaxPooling1D()(conv3)
    concat = tf.keras.layers.Concatenate()([pool1, pool2, pool3])
    dense = tf.keras.layers.Dense(128, activation='relu')(concat)
    dropout = tf.keras.layers.Dropout(0.5)(dense)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)
    model = tf.keras.Model(inputs=input_layer, outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Strategy
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
    print("Running on TPU")
except ValueError:
    strategy = tf.distribute.get_strategy()
    print("Running on GPU or CPU")

# Train model
with strategy.scope():
    model = create_model()

history = model.fit(
    train_data, train_labels,
    epochs=5,
    batch_size=128,
    validation_data=(val_data, val_labels)
)

# Evaluate
val_loss, val_accuracy = model.evaluate(val_data, val_labels)
print(f"Validation Accuracy: {val_accuracy:.4f}")
plot_history(history)

# Predictions and metrics
val_predictions = model.predict(val_data)
val_pred_labels = (val_predictions > 0.5).astype(int).flatten()
cm = confusion_matrix(val_labels, val_pred_labels)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

fpr, tpr, _ = roc_curve(val_labels, val_predictions)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()
